{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis Hotel Booking Demand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_functions import *\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge xgboost\n",
    "# pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('hotel_bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis I (EDA I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts_csv(df_original)\n",
    "count_nulls(df_original)\n",
    "hist_boxplot(df_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119390, 32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows and columns\n",
    "df_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(column):\n",
    "    upper = column.max()\n",
    "    lower = column.min()\n",
    "    y = (column - lower)/(upper-lower)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - First Remarks and ideas for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style='color:Orange'> **hotel** </span>: 2 classes, 66% City Hotel, 33% Resort Hotel\n",
    "- <span style='color:Orange'> **is_canceled** </span> (target_label): 2 values, 0 (62%) or 1 (38%). It is quite unbalanced\n",
    "- <span style='color:Orange'> **lead_time** </span>: many values, grouping by new categories? Skewed distributions towards the 0 value\n",
    "- <span style='color:Orange'> **arrival_date_year** </span>: 3 years, 2015, 2016 and 2017\n",
    "- <span style='color:Orange'> **arrival_date_month** </span>: many arrivals in June and July, it makes sense\n",
    "- <span style='color:Orange'> **arrival_date_week_number** </span>: probably we can combine it with arrival_date_month. It has a quite normal distribution\n",
    "- <span style='color:Orange'> **arrival_date_day_of_month** </span>: considering we have week_number and month_number we can eliminate this label\n",
    "- <span style='color:Orange'> **stays_in_weekend_nights** </span>: skewed distribution towards 0, we could eliminate some big values\n",
    "- <span style='color:Orange'> **stays_in_week_nights** </span>: same thought of previous label\n",
    "- <span style='color:Orange'> **adults** </span>: 2 is 75% and 1 is 19%, there are some values with 0\n",
    "- <span style='color:Orange'> **children** </span>: 93% has 0 children, we can think of eliminate this label or consider to create only a binary class, 4 <span style='color:Yellow'> **null** </span> values\n",
    "- <span style='color:Orange'> **babies** </span>: 99% has 0 babies, same thought of children\n",
    "- <span style='color:Orange'> **meal** </span>: 3 relevant classes, BB 77%, HB 12% and SC 8%\n",
    "- <span style='color:Orange'> **country** </span>: 6 relevant classes, PRT 41%, GBR 10%, FRA 9%, ESP 7%, DEU 6%, ITA 3%, 0.4% of <span style='color:Yellow'> **null** </span> values\n",
    "- <span style='color:Orange'> **market_segment** </span>: 5 relevant classes, Online TA 47%, Offline TA/TO 20%, Groups 17%, Direct 10% and Corporate 4.4%\n",
    "- <span style='color:Orange'> **distribution_channel** </span>: 3 relevant classes TA/TO 82%, Direct 12% and Corporate 5%\n",
    "- <span style='color:Orange'> **is_repeated_guest** </span>: the clients are quite new, 0 is 97% and 1 is 3%\n",
    "- <span style='color:Orange'> **previous_cancellations** </span>: two relevant classes, 0 is about 95% and 1 is about 5%. We could transform this into a binary label\n",
    "- <span style='color:Orange'> **previous_bookings_not_canceled** </span>: it is not relevant since we have previous cancellations.\n",
    "- <span style='color:Orange'> **reserved_room_type** </span>: 4 relevant classes, A is 72%, D is 16%, E is 5%, F is 2.4 %\n",
    "- <span style='color:Orange'> **assigned_room_type** </span>: 3 relevant classes, A is 62%, B 21% and E 6%. It could be interesting to know when the customer knows the **assigned_room_type**. If they knew this online some time before, it could be a reason for cancellation\n",
    "- <span style='color:Orange'> **booking_changes** </span>: distribution is skewed towards 0, 3 relevant classes, 0 is 85%, 1 is 10% and 2 is 3%. More changes imply a high probable cancellation?\n",
    "- <span style='color:Orange'> **deposit_type** </span>: only 3 classes, No deposit 87%, Non Refund 12% and Refundable 1%. We could Merge Refundable with No deposit? Non refund implies no cancellation?\n",
    "- <span style='color:Orange'> **agent** </span>: too many values, it seems to be without important information, 13% <span style='color:Yellow'> **nulls** </span>. We can cancel this label\n",
    "- <span style='color:Orange'> **company** </span>: 94% of rows are <span style='color:Yellow'> **null** </span>\n",
    "- <span style='color:Orange'> **days_in_waiting_list** </span>: there are many values but 97% of rows is 0. Binary label?\n",
    "- <span style='color:Orange'> **customer_type** </span>: only 4 classes, Transient is 75%, Transient-Party 21%, Contract is about 3.5% and Group is about 0.5%\n",
    "- <span style='color:Orange'> **adr** </span>: there are some annoying outliers, we need to group the values somehow\n",
    "- <span style='color:Orange'> **required_car_parking_spaces** </span>: only two relevant classes, 0 is about 94% and 1 is about 6%. Trasnform it in a binary label\n",
    "- <span style='color:Orange'> **total_of_special_requests** </span>: only 3 relevant classes, 0 is 59%, 1 is 28% and 2 is 10%. Maybe we could group the information\n",
    "- <span style='color:Orange'> **reservation_status** </span>: this is a posteriori information, we should eliminate it\n",
    "- <span style='color:Orange'> **reservation_status_date** </span>: non-important information, eliminate this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Grouping, Treating outliers and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>distribution_channel</th>\n",
       "      <th>Corporate</th>\n",
       "      <th>Direct</th>\n",
       "      <th>GDS</th>\n",
       "      <th>TA/TO</th>\n",
       "      <th>Undefined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market_segment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aviation</th>\n",
       "      <td>227.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complementary</th>\n",
       "      <td>89.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corporate</th>\n",
       "      <td>4788.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct</th>\n",
       "      <td>90.0</td>\n",
       "      <td>12276.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Groups</th>\n",
       "      <td>1228.0</td>\n",
       "      <td>1472.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17111.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Offline TA/TO</th>\n",
       "      <td>212.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>23946.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online TA</th>\n",
       "      <td>43.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>56153.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undefined</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "distribution_channel  Corporate   Direct    GDS    TA/TO  Undefined\n",
       "market_segment                                                     \n",
       "Aviation                  227.0      NaN    NaN     10.0        NaN\n",
       "Complementary              89.0    576.0    NaN     78.0        NaN\n",
       "Corporate                4788.0    172.0    NaN    335.0        NaN\n",
       "Direct                     90.0  12276.0    1.0    237.0        2.0\n",
       "Groups                   1228.0   1472.0    NaN  17111.0        NaN\n",
       "Offline TA/TO             212.0     16.0   45.0  23946.0        NaN\n",
       "Online TA                  43.0    133.0  147.0  56153.0        1.0\n",
       "Undefined                   NaN      NaN    NaN      NaN        2.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_original.copy()\n",
    "\n",
    "# lead_time column\n",
    "df['lead_time'] = np.where(df['lead_time']>365, 365, df['lead_time'])\n",
    "\n",
    "# arrival_date_month\n",
    "df.drop('arrival_date_month', axis = 1, inplace = True)\n",
    "\n",
    "# arrival_date_week_number\n",
    "# Grouping considering first half of the month and second half\n",
    "df['arrival_date_week_number'] = (df['arrival_date_week_number']-1) // 2\n",
    "\n",
    "# arrival_date_day_of_month\n",
    "df.drop('arrival_date_day_of_month', axis = 1, inplace = True)\n",
    "\n",
    "# stays_in_weekend_nights\n",
    "df['stays_in_weekend_nights'] = np.where(df['stays_in_weekend_nights']>2, 3, df['stays_in_weekend_nights'])\n",
    "\n",
    "# stays_in_week_nights\n",
    "df['stays_in_week_nights'] = np.where(df['stays_in_week_nights']>5, 6, df['stays_in_week_nights'])\n",
    "\n",
    "# adults\n",
    "df['adults'] = np.where(df['adults']>3, 3, df['adults'])\n",
    "df['adults'].replace(0, df['adults'].median(), inplace = True)\n",
    "\n",
    "# children\n",
    "df['children'] = np.where(df['children']>0, 1, df['children'])\n",
    "df['children'].replace(np.nan, 0, inplace = True)\n",
    "\n",
    "# babies\n",
    "df.drop('babies', axis = 1, inplace = True)\n",
    "\n",
    "# meal\n",
    "# We merge Undefined with SC (no service)\n",
    "# and full breakfast with half breakfast\n",
    "df['meal'].replace({'Undefined': 'SC', 'FB':'HB'}, inplace = True)\n",
    "\n",
    "# country\n",
    "countries = pd.read_csv('value_counts_csv/country.csv')\n",
    "top_country = list(countries['country'].head(6))\n",
    "df['country'] = np.where(df['country'].isin(top_country), df['country'], 'Others')\n",
    "\n",
    "# market_segment and distribution_channel\n",
    "# Further EDA for studying the relation between \n",
    "# market_segment and distribution_channel, they seem similar\n",
    "df.groupby(['market_segment', 'distribution_channel']).size().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style='color:Orange'> **Market segment** </span> seems to be a specialization of <span style='color:Orange'> **distribution channel** </span>. We decide to keep the former and drop the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('distribution_channel', axis = 1, inplace = True)\n",
    "\n",
    "market_segments = pd.read_csv('value_counts_csv/market_segment.csv')\n",
    "top_segments = list(market_segments['market_segment'].head(4))\n",
    "df['market_segment'] = np.where(df['market_segment'].isin(top_segments), df['market_segment'], 'Others')\n",
    "\n",
    "# previous_cancellations\n",
    "df['previous_cancellations'] = np.where(df['previous_cancellations']>1, 1, 0)\n",
    "\n",
    "# previous_bookings_not_canceled\n",
    "df.drop('previous_bookings_not_canceled', axis = 1, inplace = True)\n",
    "\n",
    "# reserved_room_type\n",
    "room_types = pd.read_csv('value_counts_csv/reserved_room_type.csv')\n",
    "top_room = list(room_types['reserved_room_type'].head(4))\n",
    "df['reserved_room_type'] = np.where(df['reserved_room_type'].isin(top_room), df['reserved_room_type'], 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We interpret the assigned_room_type label as assigned a posteriori. Therefore we decide to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigned_room_type\n",
    "df.drop('assigned_room_type', axis = 1, inplace = True)\n",
    "\n",
    "# booking_changes\n",
    "df['booking_changes'] = np.where(df['booking_changes']>2, 2, df['booking_changes'])\n",
    "\n",
    "# deposit_type\n",
    "df['deposit_type'].replace('Refundable', 'No Deposit', inplace = True)\n",
    "\n",
    "# agent\n",
    "df.drop('agent', axis = 1, inplace = True)\n",
    "\n",
    "# company\n",
    "df.drop('company', axis = 1, inplace = True)\n",
    "\n",
    "# days_in_waiting_list\n",
    "df['days_in_waiting_list'] = np.where(df['days_in_waiting_list']>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are confused about the interpretation of the <font color='orange'>**adr**</font> label and we decide not to consider it. We are not sure if it includes some a posteriori information. We can come back to it in further phase if some new interpreation arises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_car_parking_spaces\n",
    "df['required_car_parking_spaces'] = np.where(df['required_car_parking_spaces']>0, 1, 0)\n",
    "\n",
    "# total_of_special_requests\n",
    "df['total_of_special_requests'] = np.where(df['total_of_special_requests']>1, 2, df['total_of_special_requests'])\n",
    "\n",
    "# reservation_status and reservation_status_date\n",
    "df.drop(['reservation_status', 'reservation_status_date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119390, 22)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check how many columns we have\n",
    "# after the droppings\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Handling Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to treat the 7 categorical features that we have: <span style='color:Orange'> **hotel** </span>, <span style='color:Orange'> **meal** </span>, <span style='color:Orange'> **country** </span>, <span style='color:Orange'> **market_segment** </span>, <span style='color:Orange'> **reserved_room_type** </span>, <span style='color:Orange'> **deposit_type** </span> and <span style='color:Orange'> **customer_type** </span> . For the moment we consider them as **ordinal categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_dummies(df):\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    \n",
    "    cat_cols = df.columns[df.dtypes==object].tolist()\n",
    "    df = pd.get_dummies(df, prefix = cat_cols, columns = cat_cols)\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cat_dummies(df)\n",
    "#df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119390, 39)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check how many columns we have\n",
    "# after get_dummies\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max transformation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "df[df.columns] = min_max.fit_transform(df[df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Correlations and Associations Study"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "numerical vs numerical: Pearson and Spearman\n",
    "categorical vs numerical: eta correlation (anova test)\n",
    "categorical vs categorical: cramer's v (chi-squared test)\n",
    "check if our metrics are symmetric: correlation and cramer yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                df        sum_sq       mean_sq           F         PR(>F)\n",
      "C(hotel)       1.0  5.870169e+06  5.870169e+06  582.742785  1.933550e-128\n",
      "Residual  119388.0  1.202636e+09  1.007334e+04         NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "#anova_model = ols('lead_time ~ C(hotel)', df).fit()\n",
    "#anova_results = anova_lm(anova_model)\n",
    "#print(anova_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_col_2 = 'lead_time'\n",
    "#name_col_1 = 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                df        sum_sq       mean_sq           F         PR(>F)\n",
      "C(hotel)       1.0  5.870169e+06  5.870169e+06  582.742785  1.933550e-128\n",
      "Residual  119388.0  1.202636e+09  1.007334e+04         NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "#anova_model = ols(name_col_2+str(' ~ C(')+str(name_col_1)+str(')'), df).fit()\n",
    "#anova_results = anova_lm(anova_model)\n",
    "#print(anova_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, method_numeric='pearson'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Comment here\n",
    "    \n",
    "    \"\"\"\n",
    "    l = df.shape[1]\n",
    "    matrix = np.zeros(shape=(l,l))\n",
    "\n",
    "    for j in range(l):\n",
    "        col_1 = df.iloc[:,j]\n",
    "        name_col_1 = df.columns[j]\n",
    "        \n",
    "        for i in range(j,l):\n",
    "            col_2 = df.iloc[:,i]\n",
    "            name_col_2 = df.columns[i]\n",
    "            \n",
    "            if is_numeric_dtype(col_1) and is_numeric_dtype(col_2):\n",
    "                correlation = col_1.corr(col_2)\n",
    "                matrix[j,i] = matrix[i,j] = correlation\n",
    "                \n",
    "            elif is_string_dtype(col_1) and is_string_dtype(col_2):\n",
    "                frequency_table = pd.crosstab(col_1, col_2)\n",
    "                \n",
    "                chi2 = chi2_contingency(frequency_table)[0]\n",
    "                \n",
    "                total_frequencies = df.shape[0]\n",
    "                phi2 = chi2/total_frequencies\n",
    "                r, k = frequency_table.shape\n",
    "                cramer = np.sqrt(phi2/min((r-1),(k-1)))\n",
    "                matrix[i,j] = matrix[j,i] = cramer\n",
    "             \n",
    "            elif is_string_dtype(col_1) and is_numeric_dtype(col_2):\n",
    "                anova_model = ols(name_col_2 + str(' ~ C(') + name_col_1 + str(')'), df).fit()\n",
    "                anova_results = anova_lm(anova_model)\n",
    "                eta2 = anova_results['sum_sq'][0]/(anova_results['sum_sq'][0]+anova_results['sum_sq'][1])\n",
    "                matrix[i,j] = matrix[j,i] = eta2\n",
    "            \n",
    "            elif is_numeric_dtype(col_1) and is_string_dtype(col_2):\n",
    "                anova_model = ols(name_col_1 + str(' ~ C(') + name_col_2 + str(')'), df).fit()\n",
    "                anova_results = anova_lm(anova_model)\n",
    "                eta2 = anova_results['sum_sq'][0]/(anova_results['sum_sq'][0]+anova_results['sum_sq'][1])\n",
    "                matrix[i,j] = matrix[j,i] = eta2\n",
    "            \n",
    "            else: \n",
    "                matrix[i,j] = matrix[j,i] = np.nan\n",
    "                \n",
    "                \n",
    "    return(matrix)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def correlation_ratio(categories, measurements):\n",
    "        #fcat, _ = pd.factorize(categories)\n",
    "        #cat_num = np.max(fcat)+1\n",
    "        #y_avg_array = np.zeros(cat_num)\n",
    "        #n_array = np.zeros(cat_num)\n",
    "        \n",
    "        #for i in range(0,cat_num):\n",
    "            #cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "            #n_array[i] = len(cat_measures)\n",
    "            #y_avg_array[i] = np.average(cat_measures)\n",
    "            \n",
    "        #y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "        #numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "        #denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "        \n",
    "        #if numerator == 0:\n",
    "            #eta = 0.0\n",
    "        #else:\n",
    "            #eta = numerator/denominator\n",
    "        #return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cramers_v(x, y):\n",
    "    #confusion_matrix = pd.crosstab(x,y)\n",
    "    #chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    #n = confusion_matrix.sum().sum()\n",
    "    #phi2 = chi2/n\n",
    "    #r,k = confusion_matrix.shape\n",
    "    #phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    #rcorr = r-((r-1)**2)/(n-1)\n",
    "    #kcorr = k-((k-1)**2)/(n-1)\n",
    "    #return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.99981216e-01,  1.86407875e-02,  4.85737415e-03,\n",
       "         1.24379232e-03,  4.67798339e-07,  3.22333506e-02,\n",
       "         5.57368816e-02,  4.17408768e-06,  1.70625362e-03,\n",
       "         2.38880887e-01,  2.42879340e-01,  1.39342654e-01,\n",
       "         2.54225157e-03,  2.51215288e-04,  2.80407564e-01,\n",
       "         6.58898252e-03,  1.71976309e-01,  1.01886191e-02,\n",
       "         5.18445574e-02,  9.35465091e-03,  4.86652126e-02,\n",
       "         2.00626643e-03],\n",
       "       [ 1.86407875e-02,  1.00000000e+00,  2.95484731e-01,\n",
       "         1.66598602e-02,  8.02228485e-03, -3.68982941e-03,\n",
       "         2.79385551e-02,  6.25024250e-02, -3.75047306e-03,\n",
       "         1.73060372e-04,  1.18512818e-01,  7.10359459e-02,\n",
       "        -8.47934184e-02,  1.95073645e-02,  5.13378316e-03,\n",
       "        -1.68178957e-01,  2.31800836e-01,  9.90324376e-02,\n",
       "         1.86154968e-02,  4.75565979e-02, -1.97398821e-01,\n",
       "        -2.42727497e-01],\n",
       "       [ 4.85737415e-03,  2.95484731e-01,  1.00000000e+00,\n",
       "         3.27957985e-02,  1.30302449e-01,  9.96279973e-02,\n",
       "         2.03907877e-01,  1.36480344e-01, -3.52430450e-02,\n",
       "         2.69886557e-02,  3.88055026e-02,  1.93303335e-01,\n",
       "        -1.28551268e-01,  2.11279772e-03,  1.06404690e-02,\n",
       "        -1.47543599e-03,  1.36612947e-01,  1.46930370e-01,\n",
       "         3.63859412e-02, -5.65969113e-02, -1.20470993e-01,\n",
       "        -9.99113472e-02],\n",
       "       [ 1.24379232e-03,  1.66598602e-02,  3.27957985e-02,\n",
       "         1.00000000e+00, -5.40546746e-01,  2.16981509e-02,\n",
       "         3.50428448e-02,  4.71596852e-02,  5.92959490e-02,\n",
       "         1.87412626e-02,  4.38327475e-02,  3.96482462e-02,\n",
       "         1.03413169e-02, -3.12729410e-02,  9.54261416e-03,\n",
       "         3.10474337e-02,  4.35113230e-03, -7.70796904e-02,\n",
       "         6.26966364e-02,  1.97580088e-01, -1.48626862e-02,\n",
       "         1.08622715e-01],\n",
       "       [ 4.67798339e-07,  8.02228485e-03,  1.30302449e-01,\n",
       "        -5.40546746e-01,  1.00000000e+00,  2.57833658e-02,\n",
       "         2.85769479e-02,  2.76687357e-02,  7.13723145e-03,\n",
       "         3.57260200e-03,  1.68062750e-03,  4.98755341e-03,\n",
       "        -2.99106163e-02,  1.09237679e-03,  2.18035916e-04,\n",
       "         7.05849647e-03,  5.86033557e-05, -3.39344399e-03,\n",
       "         1.08791970e-02,  7.60713176e-02,  2.98364307e-03,\n",
       "         2.51413556e-02],\n",
       "       [ 3.22333506e-02, -3.68982941e-03,  9.96279973e-02,\n",
       "         2.16981509e-02,  2.57833658e-02,  1.00000000e+00,\n",
       "         3.52439950e-01,  1.23138081e-01,  5.31397777e-02,\n",
       "         1.12879424e-02,  3.98791755e-02,  2.35837382e-02,\n",
       "        -9.45958697e-02, -1.05049473e-02,  2.35787324e-02,\n",
       "         4.59942049e-02,  1.38967504e-02, -7.44126280e-02,\n",
       "         1.16347282e-02,  6.23355821e-02, -1.91017771e-02,\n",
       "         7.71184301e-02],\n",
       "       [ 5.57368816e-02,  2.79385551e-02,  2.03907877e-01,\n",
       "         3.50428448e-02,  2.85769479e-02,  3.52439950e-01,\n",
       "         1.00000000e+00,  1.41075300e-01,  5.78493454e-02,\n",
       "         1.73857304e-02,  4.87032483e-02,  3.03801868e-02,\n",
       "        -1.17389624e-01, -1.27249610e-02,  3.83977606e-02,\n",
       "         7.60951993e-02,  6.68764016e-03, -5.18526526e-03,\n",
       "         1.75634602e-02,  9.55292268e-02, -2.68519684e-02,\n",
       "         7.89831584e-02],\n",
       "       [ 4.17408768e-06,  6.25024250e-02,  1.36480344e-01,\n",
       "         4.71596852e-02,  2.76687357e-02,  1.23138081e-01,\n",
       "         1.41075300e-01,  1.00000000e+00,  8.81962426e-02,\n",
       "         3.20262417e-03,  2.55346161e-02,  1.08656567e-01,\n",
       "        -1.68836940e-01, -3.76098735e-02,  7.42933525e-02,\n",
       "        -5.97084344e-02,  1.43929573e-03, -3.94028019e-02,\n",
       "         1.86434027e-02,  2.76716261e-01,  1.65961158e-02,\n",
       "         1.62394944e-01],\n",
       "       [ 1.70625362e-03, -3.75047306e-03, -3.52430450e-02,\n",
       "         5.92959490e-02,  7.13723145e-03,  5.31397777e-02,\n",
       "         5.78493454e-02,  8.81962426e-02,  1.00000000e+00,\n",
       "         5.13936115e-03,  7.88282030e-03,  3.77082726e-02,\n",
       "        -3.46933644e-02, -1.51810519e-02,  2.77935597e-01,\n",
       "         6.54119001e-02,  1.06675772e-02, -4.80965412e-02,\n",
       "         1.08586184e-02,  3.09920986e-01,  6.10268401e-02,\n",
       "         1.01827315e-01],\n",
       "       [ 2.38880887e-01,  1.73060372e-04,  2.69886557e-02,\n",
       "         1.87412626e-02,  3.57260200e-03,  1.12879424e-02,\n",
       "         1.73857304e-02,  3.20262417e-03,  5.13936115e-03,\n",
       "         1.00000000e+00,  1.46696817e-01,  2.19597874e-01,\n",
       "         3.59981677e-03,  6.25893640e-04,  1.35390730e-01,\n",
       "         4.91254837e-03,  1.10535818e-01,  2.33645262e-03,\n",
       "         1.35980425e-01,  1.84642949e-02,  2.74953503e-03,\n",
       "         3.84546944e-03],\n",
       "       [ 2.42879340e-01,  1.18512818e-01,  3.88055026e-02,\n",
       "         4.38327475e-02,  1.68062750e-03,  3.98791755e-02,\n",
       "         4.87032483e-02,  2.55346161e-02,  7.88282030e-03,\n",
       "         1.46696817e-01,  1.00000000e+00,  2.22171330e-01,\n",
       "         2.72390468e-02,  3.22089098e-03,  1.01630044e-01,\n",
       "         5.65009959e-03,  4.29099242e-01,  1.79930117e-02,\n",
       "         1.04022411e-01,  3.55171227e-02,  9.23947367e-03,\n",
       "         5.16795900e-02],\n",
       "       [ 1.39342654e-01,  7.10359459e-02,  1.93303335e-01,\n",
       "         3.96482462e-02,  4.98755341e-03,  2.35837382e-02,\n",
       "         3.03801868e-02,  1.08656567e-01,  3.77082726e-02,\n",
       "         2.19597874e-01,  2.22171330e-01,  1.00000000e+00,\n",
       "         1.19991788e-01,  1.12007290e-02,  1.71064509e-01,\n",
       "         1.39208093e-02,  5.24065355e-01,  5.20330611e-02,\n",
       "         2.73768446e-01,  1.32132095e-01,  3.39840332e-02,\n",
       "         1.95477815e-01],\n",
       "       [ 2.54225157e-03, -8.47934184e-02, -1.28551268e-01,\n",
       "         1.03413169e-02, -2.99106163e-02, -9.45958697e-02,\n",
       "        -1.17389624e-01, -1.68836940e-01, -3.46933644e-02,\n",
       "         3.59981677e-03,  2.72390468e-02,  1.19991788e-01,\n",
       "         1.00000000e+00,  1.74544834e-01,  1.33782143e-03,\n",
       "         1.56612730e-02,  3.41324405e-03, -2.75092323e-02,\n",
       "         1.10400054e-02, -1.34314447e-01,  7.52890566e-02,\n",
       "         5.87329934e-03],\n",
       "       [ 2.51215288e-04,  1.95073645e-02,  2.11279772e-03,\n",
       "        -3.12729410e-02,  1.09237679e-03, -1.05049473e-02,\n",
       "        -1.27249610e-02, -3.76098735e-02, -1.51810519e-02,\n",
       "         6.25893640e-04,  3.22089098e-03,  1.12007290e-02,\n",
       "         1.74544834e-01,  1.00000000e+00,  4.83112780e-04,\n",
       "        -4.77196796e-03,  6.75691313e-04,  1.17326413e-02,\n",
       "         1.39086837e-04, -5.05750085e-02,  2.36908145e-03,\n",
       "         2.24716685e-03],\n",
       "       [ 2.80407564e-01,  5.13378316e-03,  1.06404690e-02,\n",
       "         9.54261416e-03,  2.18035916e-04,  2.35787324e-02,\n",
       "         3.83977606e-02,  7.42933525e-02,  2.77935597e-01,\n",
       "         1.35390730e-01,  1.01630044e-01,  1.71064509e-01,\n",
       "         1.33782143e-03,  4.83112780e-04,  1.00000000e+00,\n",
       "         6.81483943e-03,  2.13988898e-01,  9.75663455e-03,\n",
       "         1.01394717e-01,  1.51242859e-01,  2.05456834e-02,\n",
       "         2.58682626e-02],\n",
       "       [ 6.58898252e-03, -1.68178957e-01, -1.47543599e-03,\n",
       "         3.10474337e-02,  7.05849647e-03,  4.59942049e-02,\n",
       "         7.60951993e-02, -5.97084344e-02,  6.54119001e-02,\n",
       "         4.91254837e-03,  5.65009959e-03,  1.39208093e-02,\n",
       "         1.56612730e-02, -4.77196796e-03,  6.81483943e-03,\n",
       "         1.00000000e+00,  1.90698537e-02, -1.96515546e-02,\n",
       "         1.46398815e-02,  2.53402135e-02,  7.55078115e-02,\n",
       "         4.78811723e-02],\n",
       "       [ 1.71976309e-01,  2.31800836e-01,  1.36612947e-01,\n",
       "         4.35113230e-03,  5.86033557e-05,  1.38967504e-02,\n",
       "         6.68764016e-03,  1.43929573e-03,  1.06675772e-02,\n",
       "         1.10535818e-01,  4.29099242e-01,  5.24065355e-01,\n",
       "         3.41324405e-03,  6.75691313e-04,  2.13988898e-01,\n",
       "         1.90698537e-02,  9.99960952e-01,  5.35942365e-02,\n",
       "         1.25330803e-01,  7.67515754e-03,  9.19783570e-03,\n",
       "         7.96093737e-02],\n",
       "       [ 1.01886191e-02,  9.90324376e-02,  1.46930370e-01,\n",
       "        -7.70796904e-02, -3.39344399e-03, -7.44126280e-02,\n",
       "        -5.18526526e-03, -3.94028019e-02, -4.80965412e-02,\n",
       "         2.33645262e-03,  1.79930117e-02,  5.20330611e-02,\n",
       "        -2.75092323e-02,  1.17326413e-02,  9.75663455e-03,\n",
       "        -1.96515546e-02,  5.35942365e-02,  1.00000000e+00,\n",
       "         1.12530939e-02, -4.20825385e-02, -4.34066660e-02,\n",
       "        -1.17644271e-01],\n",
       "       [ 5.18445574e-02,  1.86154968e-02,  3.63859412e-02,\n",
       "         6.26966364e-02,  1.08791970e-02,  1.16347282e-02,\n",
       "         1.75634602e-02,  1.86434027e-02,  1.08586184e-02,\n",
       "         1.35980425e-01,  1.04022411e-01,  2.73768446e-01,\n",
       "         1.10400054e-02,  1.39086837e-04,  1.01394717e-01,\n",
       "         1.46398815e-02,  1.25330803e-01,  1.12530939e-02,\n",
       "         1.00000000e+00,  3.16892761e-02,  4.94286918e-03,\n",
       "         2.72272237e-02],\n",
       "       [ 9.35465091e-03,  4.75565979e-02, -5.65969113e-02,\n",
       "         1.97580088e-01,  7.60713176e-02,  6.23355821e-02,\n",
       "         9.55292268e-02,  2.76716261e-01,  3.09920986e-01,\n",
       "         1.84642949e-02,  3.55171227e-02,  1.32132095e-01,\n",
       "        -1.34314447e-01, -5.05750085e-02,  1.51242859e-01,\n",
       "         2.53402135e-02,  7.67515754e-03, -4.20825385e-02,\n",
       "         3.16892761e-02,  1.00000000e+00,  5.75800882e-02,\n",
       "         1.72536690e-01],\n",
       "       [ 4.86652126e-02, -1.97398821e-01, -1.20470993e-01,\n",
       "        -1.48626862e-02,  2.98364307e-03, -1.91017771e-02,\n",
       "        -2.68519684e-02,  1.65961158e-02,  6.10268401e-02,\n",
       "         2.74953503e-03,  9.23947367e-03,  3.39840332e-02,\n",
       "         7.52890566e-02,  2.36908145e-03,  2.05456834e-02,\n",
       "         7.55078115e-02,  9.19783570e-03, -4.34066660e-02,\n",
       "         4.94286918e-03,  5.75800882e-02,  1.00000000e+00,\n",
       "         8.51106248e-02],\n",
       "       [ 2.00626643e-03, -2.42727497e-01, -9.99113472e-02,\n",
       "         1.08622715e-01,  2.51413556e-02,  7.71184301e-02,\n",
       "         7.89831584e-02,  1.62394944e-01,  1.01827315e-01,\n",
       "         3.84546944e-03,  5.16795900e-02,  1.95477815e-01,\n",
       "         5.87329934e-03,  2.24716685e-03,  2.58682626e-02,\n",
       "         4.78811723e-02,  7.96093737e-02, -1.17644271e-01,\n",
       "         2.72272237e-02,  1.72536690e-01,  8.51106248e-02,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function correlation_matrix in module __main__:\n",
      "\n",
      "correlation_matrix(df, method_numeric='pearson')\n",
      "    Comment here\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python setup.py install--user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.copy()\n",
    "df_model = df_model.select_dtypes(exclude=['object'])\n",
    "df_model = pd.get_dummies(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop('is_canceled', axis = 1)\n",
    "y = df_model['is_canceled']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 2} \n",
    "\n",
    "steps = 20  # The number of training iterations\n",
    "\n",
    "model = xgb.train(param, D_train, steps)\n",
    "\n",
    "preds = model.predict(D_test)\n",
    "print(preds)\n",
    "\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(best_preds)\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "parameters = {\n",
    "     \"eta\"    : [0.20, 0.30] ,\n",
    "     \"max_depth\"        : [ 3, 6],\n",
    "     \"gamma\"            : [ 0.1, 0.2]\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(xgb_clf,\n",
    "                    parameters,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "model = grid.fit(X_train, Y_train)\n",
    "#predictions = model.predict(X_test)\n",
    "\n",
    "#model.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_\n",
    "model.best_estimator_\n",
    "cvres = model.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "#brute force scan for all parameters, here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=StratifiedKFold(train['QuoteConversion_Flag'], n_folds=5, shuffle=True), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clf.fit(train[features], train[\"QuoteConversion_Flag\"])\n",
    "\n",
    "#trust your CV!\n",
    "best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "print('Raw AUC score:', score)\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "test_probs = clf.predict_proba(test[features])[:,1]\n",
    "\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "sample.QuoteConversion_Flag = test_probs\n",
    "sample.to_csv(\"xgboost_best_parameter_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_gridCV(X_train, y_train, clf, \n",
    "                      X_test = None, y_test = None, cv = 3, scoring = 'accuracy', params = {}, model_name = \"model\"):\n",
    "    \n",
    "    grid = GridSearchCV(clf, params, cv = cv, scoring = scoring, refit = True)\n",
    "    \n",
    "    model = grid.fit(X_train, y_train)\n",
    "    print(\"The best parameters of grid are: \", model.best_params_, \n",
    "          \"\\nThe best estimator is: \", model.best_estimator_)\n",
    "    \n",
    "    if not os.path.exists('Models/CV_results'):\n",
    "        os.makedirs('Models/CV_results')\n",
    "    \n",
    "    cvres = model.cv_results_\n",
    "    \n",
    "    if params != {}:    \n",
    "        dataframe = pd.DataFrame(cvres[\"params\"])\n",
    "        dataframe.insert(0, \"mean_test_score\", cvres[\"mean_test_score\"])\n",
    "                        \n",
    "    else: \n",
    "        dataframe = pd.DataFrame({\"mean_test_score\":cvres[\"mean_test_score\"]})\n",
    "            \n",
    "    dataframe.to_csv(\"./Models/CV_results/CV_results_\"+model_name+\".csv\", index = False) \n",
    "    \n",
    "    if X_test is not None:\n",
    "        results = model.predict(X_test)\n",
    "    \n",
    "        if y_test is not None:\n",
    "            print(\"Precision = {}\".format(precision_score(y_test, results, average='macro')))\n",
    "            print(\"Recall = {}\".format(recall_score(y_test, results, average='macro')))\n",
    "            print(\"Accuracy = {}\".format(accuracy_score(y_test, results)))\n",
    "            \n",
    "    return(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gridCV(X_train, Y_train, xgb.XGBClassifier(), X_test, Y_test, \n",
    "                  model_name = \"xg_boost_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
